# -*- coding: utf-8 -*-
"""
Spyder Editor

15201002 Arif Shahriar
"""

#initializing PCA
from sklearn import decomposition
pca=decomposition.PCA()
import pandas as pd
import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
data=pd.read_csv('Thesis_responses_Scaled.csv')


#configure the parameters
#number of componenets = 2
labels=data['Flag']
data=data.drop(['Timestamp','Flag','ID','Rehab'],axis=1)

#scaler=StandardScaler()
#scaler.fit(data)
## compute the mean and standard which will be used in the next command
#X_scaled=scaler.transform(data)
## fit and transform can be applied together and I leave that for simple exercise

pca.n_components=2
pca_data=pca.fit_transform(data)

#pca_reduced will contain 2-d projects of sample data
print("shape of pca_reduced.shape = ",pca_data.shape)

ex_variance=np.var(pca_data,axis=0)
ex_variance_ratio=ex_variance/np.sum(ex_variance)
print("Ex variance ratio of components ",ex_variance_ratio)

# attaching the label for each 2-d data point 
pca_data = np.vstack((pca_data.T, labels)).T

# creating a new data fram which help us in ploting the result data
pca_df = pd.DataFrame(data=pca_data, columns=("1st_principal", "2nd_principal", "label"))
sn.FacetGrid(pca_df, hue="label", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()
plt.show()

# PCA for dimensionality redcution (non-visualization)

pca.n_components = 59
pca_data = pca.fit_transform(data)

percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);

cum_var_explained = np.cumsum(percentage_var_explained)

# Plot the PCA spectrum
plt.rcParams.update({'font.size': 16})
plt.figure(1, figsize=(6, 4))

plt.clf()
plt.plot(cum_var_explained, linewidth=2)
plt.axis('tight')
plt.grid()
plt.xlabel('n_components')
plt.ylabel('Cumulative_explained_variance')
plt.show()

# If we take 30-dimensions, approx. 90% of variance is expalined.
