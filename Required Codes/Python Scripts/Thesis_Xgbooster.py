# -*- coding: utf-8 -*-
"""
Created on Mon Aug  5 23:06:40 2019

@author: Arif Shahriar 15201002
"""

import xgboost as xgb
import pandas as pd
from sklearn.metrics import mean_squared_error
import numpy as np

#data=pd.read_csv("Alpha_Test.csv")
#
#print(data.head())
#X=data.iloc[:,0:40].values
#y=data.iloc[:,40].values

data=pd.read_csv('Thesis_responses_Scaled.csv')

X=data.iloc[:,1:60].values
y=data.Flag.values

data_dmatrix=xgb.DMatrix(data=X,label=y)

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)

xg_classifier=xgb.XGBClassifier(objective='reg:linear',colsample_bytree=0.3,learning_rate=0.1,max_depth=30,alpha=10,n_estimators=56)

xg_classifier.fit(X_train,y_train)
y_preds=xg_classifier.predict(X_test)

rmse=np.sqrt(mean_squared_error(y_test,y_preds))
print("RMSE: %f" % (rmse))

from sklearn.metrics import classification_report,accuracy_score,confusion_matrix,matthews_corrcoef
print(accuracy_score(y_test,y_preds))
print(classification_report(y_test,y_preds))
 
#ROC curve and report
cm=confusion_matrix(y_test, y_preds)
print(classification_report(y_test,y_preds))
print("The Matthews correlation coefficient ",matthews_corrcoef(y_test,y_preds))
print(cm)
tn=cm[0,0]
fp=cm[0,1]
fn=cm[1,0]
tp=cm[1,1]
print('Sensitivity:',tp/(tp+fn))
print('Specificity:',tn/(tn+fp))
print('Precision:',tp/(tp+fp))
print('Negative predictive value:',tn/(tn+fn))
print('Accuracy=',((tn+tp)*100)/(tn+tp+fn+fp))

from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
fpr=dict()
tpr=dict()
roc_auc=dict()
fpr[0],tpr[0],_=roc_curve(y_test,y_preds)
roc_auc[0]=auc(fpr[0],tpr[0])

fpr["micro"],tpr["micro"],_=roc_curve(y_test.ravel(),y_preds.ravel())
roc_auc["micro"]=auc(fpr["micro"],tpr["micro"])

plt.figure()
lw = 2
plt.plot(fpr[0], tpr[0], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic of XGBoost')
plt.legend(loc="lower right")
plt.show()
print('Accuracy=',((tn+tp)*100)/(tn+tp+fn+fp))

params = {"objective":"reg:linear",'colsample_bytree': 0.3,'learning_rate': 0.1,
                'max_depth': 5, 'alpha': 10}
xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)
#import matplotlib.pyplot as plt
#
#xgb.plot_tree(xg_reg,num_trees=0)
#plt.rcParams['figure.figsize'] = [50, 10]
#plt.show()


print("40-->",data.columns.values[40])
print("44-->",data.columns.values[44])
print("48-->",data.columns.values[48])
print("55-->",data.columns.values[55])